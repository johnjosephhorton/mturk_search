\documentclass{acm_proc_article-sp}

\usepackage{ifthen,hyperref,subfig} 

\begin{document}

\title{Task Search in a Human Computation Market} 

\numberofauthors{4}

\author{
\alignauthor 
Lydia B. Chilton\\
       \affaddr{University of Washington}\\
      % \affaddr{AC101 Paul G. Allen Center, Box 352350}\\
      % \affaddr{185 Stevens Way}\\
      % \affaddr{Seattle, WA 98195-2350}\\
       \affaddr{hmslydia@cs.washington.edu}
\alignauthor 
John J. Horton\\
       \affaddr{Harvard University}\\
      % \affaddr{383 Pforzheimer Mail Center}\\
      % \affaddr{56 Linnaean Street}\\
      % \affaddr{Cambridge, MA 02138}\\
       \affaddr{horton@fas.havard.edu}
\and
\alignauthor
Robert C. Miller\\
       \affaddr{MIT CSAIL}\\
      % \affaddr{383 Pforzheimer Mail Center}\\
      % \affaddr{56 Linnaean Street}\\
      % \affaddr{Cambridge, MA 02138}\\
       \affaddr{rcm@mit.edu}
% 2nd. autho
\alignauthor
Shiri Azenkot\\
       \affaddr{University of Washington}\\
      % \affaddr{383 Pforzheimer Mail Center}\\
      % \affaddr{56 Linnaean Street}\\
      % \affaddr{Cambridge, MA 02138}\\
       \affaddr{shiri@cs.washington.edu}
% 2nd. author
}

\maketitle

\begin{abstract} 
In order to understand how a labor market for human computation
functions, it is important to know how workers search for tasks. This
paper uses two complementary methods to gain insight into how workers
search for tasks on Mechanical Turk.  First, we perform a high
frequency scrape of 36 pages of search results and analyze it by
looking at the rate of disappearance of tasks across key ways
Mechanical Turk allows workers to sort tasks. Second, we present the
results of a survey in which we paid workers for self-reported
information about how they search for tasks.  Our main findings are
that on a large scale, workers sort by which tasks are most recently
posted and which have the largest number of tasks available, and that
at least some employers try to manipulate the search results to
exploit the tendency to search for recently posted tasks. We also find
that workers look mostly at the first page of most recently posted
tasks and the first two pages of the tasks with the most available
instances but in both categories the position on the result page is
unimportant to workers. However, on an individual level, we observed
workers searching by almost all the possible categories and looking as
far as 5 pages deep into a category.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information Storage and Retrieval}{Information Search and Retrieval}
\category{J.4}{Social and Behavioral Sciences}{Economics}

\terms{Economics, Experimentation, Human Factors, Measurement}
\keywords{Search, Amazon Mechanical Turk, Human Computation, Crowdsourcing, Experimentation}


\section{Introduction}
Amazon's Mechanical Turk (MTurk) is a labor market for human
computation.  As with all markets, information plays a critical role
in determining efficiency: buyers and sellers cannot make good choices
if they do not know what their options are. According to a rational
economic model of labor supply, workers use information about the
availability and the nature of tasks to make decisions about which
tasks to accept.  If these workers lack full information about the
tasks, they are likely to make sub-optimal decisions, such as
accepting inferior offers or exiting the market when they would have
stayed, had they known about some other task. In large markets with
many buyers and sellers, lack of knowledge about all available options
is a key source of friction. This lack of knowledge stems from the
fact that search is neither perfect nor costless.

The ``search problem'' is particularly challenging in labor markets
because both jobs and workers are unique, which means that there is no
single prevailing price for a unit of labor. In many labor markets,
organizations and structures exist to help improve the quality and
quantity of information and reduce these ``search frictions,'' and we
have already seen conventional labor markets ``wired'' via job listing
and job search web sites \cite{autor2001wiring}.

In online markets, search frictions can be partly eased by making the
data searchable by providing search features.  If agents can sort and
filter tasks based on characteristics that affect the desirability of
a task, then presumably they can make better choices, improving market
efficiency. Of course, this hypothetical gain in efficiency depends
upon the search technology provided and the response of buyers to that
technology---two issues we will explore in this paper.

MTurk has several search features that allow workers to find Human
Intelligence Tasks (HITs).  HITs can be sorted by fields such as most
available, highest reward, and title.  HITs are also searchable by
keyword, minimum reward, whether they require qualifications (and
combinations thereof).  In this paper, we seek to provide insight into
how workers search for tasks.

We present the results of two methods for investigating search
behavior on MTurk.  First, we scrape pages of available HITs at a very
high rate and determine the rate at which a type of HIT is being taken
by workers.  We use a statistical model to compare the rate of HIT
disappearance across key categories of HITs that MTurk allows workers
to search by.  Our premise for observing search behavior is that
search methods which return HITs with higher rates of disappearance
are the search methods which workers use more.  This method relies
only on publicly available data---the list of HITs available on
www.mturk.com.  Second, we issued a survey on MTurk asking over 200
workers how they searched for tasks.  We posted the survey with
carefully chosen parameters so as to position it so that it can be
found more easily by some search methods and not others. This way, we
can target search behaviors that are not picked up by the analysis of
the scraped data.

In this paper, we first motivate studying search behavior in an online
human computation labor market by reviewing related work.  Next, we
describe MTurk's search features and our method of data collection
based on these search features.  We then present a statistical
analysis of our high-frequency scraped data followed by the workers
survey results.

\section{Related Work} 
In the field of human computation, it is important to ask ``why are
people contributing to this effort?" What are the motivations behind
labeling an image \cite{von2004labeling}, uploading videos to YouTube
\cite{huberman-crowdsourcing}, or identifying the genre of a song
\cite{law2003tagatune}?  On MTurk, the main motivation seems to be
money \cite{ipeirotis2010,hortonZeck2010}. However, financial
motivations do not imply that workers just try to find HITs offering
the highest reward: workers have a choice over literally thousands of
HITs that differ in their required skill, difficulty, reward and
risk. With all these factors at play, workers face a significant
decision problem. Our task is to model this decision problem.


Because MTurk is a labor market, labor economics---which models
workers as rational agents trying to maximize their net benefits over
time---might be a suitable framework. Research on the labor economics
of MTurk has shown that workers do respond positively to prices but
that price is not indicative of quality of work
\cite{mason2009fip}. Our previous work \cite{horton2010labor} shows
that not all workers follow a traditional rational model of labor
supply when choosing their level of output.  Instead of maximizing
their wage rate, many workers use target earning---they focus on
achieving a target such as \$.25 or \$1.00 and ignore the wage rate.
Although price is important to task choice, it is clearly not the only
factor.  In our earlier work, we emphasized the need for research on
search behavior to more fully understand how workers interact with the
labor market.

In other domains where users must cope with a large information space,
progress and innovation in search has had a huge positive impact.
Ideas such as PageRank \cite{brin1998pagerank} made searching the long
tail of large, linked document collections by keyword possible.
Innovations such as faceted browsing \cite{hearst2002finding} makes
search within more structured domains such as shopping faster and
easier.   Web queries are sensitive to changes in content over time
and searchers can use tools to help them find update on pages they revisit
\cite{adar2008zoetrope}. In labor markets and human computation markets, search
facilities are still evolving and it is unknown what features are most
useful in overcoming search frictions.

Web search analysis has found that by far the most clicked-on result
is the first result on the first page of results
\cite{spink2000selected}. In a market model where parties have full
information, there is no need for search technologies and there should
be no purely ``positional effects''---all parties would instantly and
costlessly find their best options. In reality, the search technology
is likely to have a strong effect on the buyer-seller matches that are
made---a point made clear by the fact that companies are willing to
spend billions of dollars to have their advertisements placed in prime
areas of the screen \cite{edelman2007internet}.

While there are similarities between any kind of online search
activity, we must be careful in generalizing.  Search behavior in one
domain does not necessarily carry over to other domains. In this paper
we focus solely on the domain of workers searching for HITs on MTurk,
using web search behavior as a guide to what features may be
important. 

\section{Summary of MTurk Search Features} \label{sec:features}
At any given time, MTurk will have on the order of 100,000 HITs
available.  HITs are presented in a list much like Google search
results.  By default, the list is sorted by ``HITs Available (most
first)".  Each page of HIT results display information for ten
HITs. The default view gives seven pieces of information:
\begin{enumerate}
\item Title (e.g., ``Choose the best category for this product")
\item Requester (e.g., ``Dolores Labs")
\item HIT Expiration Date (e.g., ``Jan 23, 2011  (38 weeks)")
\item Time Allotted (e.g., ``60 minutes")
\item Reward (e.g., ``\$0.02")
\item HITs Available (e.g., 17110)
\item Whether the worker will require a qualification to perform the HIT
\end{enumerate}
By clicking on a HIT title, the display expands to show three
additional fields:
\begin{enumerate}
\item Description (e.g., ``Decide what is the appropriate category for
  this product")
\item Keywords (e.g., ``category, categorize, product, kids")
\item Qualifications. (e.g., ``Location is US")
\end{enumerate}
The MTurk interface offers several search features placed prominently
at the top of the results page.  One of the features is a keyword
search.  Workers can search both HITs and qualifications for keywords
such as ``bonus" ``categorize" or ``Dolores Labs."  Workers can also
set a reward minimum in their search and filter out HITs for which
they are not qualified.  Additionally, HITs can be sorted in either
ascending or descending order by six categories:

\begin{enumerate}
\item HIT Creation Date ({\bfseries {\em newest}} or {\bfseries {\em oldest}})
\item HITs Available ({\bfseries {\em most}} or {\bfseries {\em fewest}})
\item Reward Amount ({\bfseries {\em highest}} or {\bfseries {\em lowest}})
\item Expiration Date ({\bfseries {\em soonest}} or {\bfseries {\em latest}})
\item Title ({\bfseries {\em a-z}} or {\bfseries {\em z-a}})
\item Time Allotted ({\bfseries {\em shortest}} or {\bfseries {\em longest}})
\end{enumerate}

Some categories change more quickly than others.  Sorting by most
recent creation date will produce a very dynamic set of HITs.
However, sorting by the most HITs Available will produce a fairly
static list because it takes a long time for a HIT with 17,000 HITs to
fall off the list.

On each worker's account summary page, MTurk suggests 10 HITs, usually
with high rewards but of no other apparent characteristics.  Other
than this, we are unaware of any services to intelligently recommend
HITs to workers.  We presume that nearly all searching is done using
the MTurk provided search interface which we have described here.

Requesters are the people that post the tasks on MTurk and pay workers
for acceptable work.  There are two basic frameworks in which
requester post tasks.  First, requesters often have a large task such
as image labeling where they need 10,000 images labeled and they welcome
the workers to do more than one of instance of the HIT.  Second, the
requester has a large task, but no user can do more than one instance
of the HIT.  This is typical of surveys---the requester wants 10,000
surveys filled out but they must be done by different people.  These
types of HITs are displayed to the worker as having only one HIT
available (because there is only one HIT available to \emph{that}
worker).  If a worker doesn't take the HIT, it will remain on his list
of available HITs until 10,000 other people have done them.  It takes
significant time to achieve the throughput of workers necessary to
complete all the HITs that show up as having only one HIT available.
It's a wonder they ever get done at all.  Because of this, we call
this important class of HITs ``1-HIT wonders."

For HITs that do allow repeat workers, the number of HITs still
remaining is always displayed.  As HITs are taken, this number
decreases.  This number can increase in only two cases: first, if a
HIT is returned by the worker and second, in the rare event that more
tasks are added to the HIT group.  For example, adding an additional
1,000 images that need to be labeled before the first 10,000 are
completed.  Fortunately, this case is easy to detect.

In this paper, we use the rate of disappearance of HITs to perform a
statistical analysis of search behavior for HITs with multiple tasks
available to all workers.  We complement that with our worker survey
where ask workers about how they search for tasks such as 1-HIT wonders that the scraping does not pick up on.

\section{Method A: Inferences from Observed Data}
We want to know if the sort category, the page a HIT is on and the
position it occupies on the page affect the disappearance of the HIT.
Our premise is that if any of these factors matter, then workers are
using those search features to find HITs. 

As a thought experiment, imagine that we could randomly move HITs to
different pages and positions within the different search categories,
without changing the attributes of the HIT. With this ability, we could
determine the causal effect of being on different pages and positions
by seeing how quickly our posted HIT was completed by workers. 

Of course, we do not actually have this ability to manipulate the
search results, but we can observe the functioning of the market and
potentially make a similar inference using the collected data. The
problem with this approach, however, is that HIT characteristics that
are likely to affect the popularity of a HIT also affect the page and
position of that HIT in the different search categories. In fact, the
relationship between attributes and search position is the whole
reason MTurk offers search features---if position in the search results
was unrelated to HIT characteristics that workers cared about, then
that search category would offer no value.

To deal with the problem of correlation between attributes and search
position, we need to make several assumptions and tailor our empirical
approach to the nature of the domain. Before we discuss our actual
model, it will be helpful to introduce a conceptual model. We assume
that the disappearance of a particular HIT during some interval of
time is a unknown function of that HIT's position and its attributes:
\[
\mbox{Disappearance of HIT $i$} = F(\mbox{position of $i$},X_i)
\]
where $X_i$ are all the HIT attributes, market level variables and
time of day effects that might also affect disappearance.  Our key
research goal is to see how manipulations of the position of
$i$---while keeping $X_i$ constant---affect our outcome measure, which
is the disappearance of HITs (i.e., work getting done). We might be
tempted to think that we could include a sufficient number of HIT
co-variates (e.g., reward, keywords etc.) and control for the effect of
$X_i$, but this is problematic, as we cannot control for all factors. 

Given that we cannot exogenously manipulate search results, and that
there are HIT attributes that we cannot control for, how can we
untangle causality? Our approach is to acknowledge the existence of
unobserved, HIT-specific idiosyncratic effects but assume that those
effects are constant over time for a particular HIT group. With this
assumption, we rely upon page and position movement by a HIT within a
search category as a ``natural experiment.''

When we observe a HIT moving from one position to another, and observe
that the HIT disappears more quickly in the new position than in the
old, then, we can conclude that the difference in popularity is
attributable to the new position, and not to some underlying change in
the nature of the HIT.

\subsection{Data Collection}
There are 12 ways MTurk allows workers to sort HITs (six categories,
each which can be sorted ascending or descending) For all 12 sort
options we scraped the first three pages of results.  Each result page
lists 10 hits, meaning that we were scraping 360 HITs each iteration.
We scraped each of the 36 pages approximately every 30 seconds---just
under the throttling rate. Although we ran the scraper for four days,
due to our own data processing constraints, in this analysis we used
data collected in a 32 hour window beginning on Thu, 29 Apr 2010
20:37:05 GMT until Sat, 01 May 2010 04:45:45 GMT. The data contains
997,322 observations. Because of the high frequency of our scraping,
each posted HIT is observed many times in data: while the data
contains nearly a million observation, we only observed 2,040 unique
HITs, with each HIT being observed, on average, a little more
than 480 times.

For each HIT, we record the ten pieces of information the MTurk
interface gives us (the seven default and the three additional---see
Section \ref{sec:features}) as well as the sort method used to find
that HIT, the page number the HIT was found on (1, 2 or 3) and the
position on that page (first through tenth).

\subsubsection{Measuring HIT disappearance}
Formally, let $s$ index the sort category, $g$ index the groups of
observations of the same HIT (which occures because the same HIT is
observed many times) and let $i$ index time-ordered observations
within a group. Let HITs be ordered from oldest to most recently
scraped, such that $i+1$ was scraped after $i$. Let the number of HITs
available for observation $i$ be $y_{igs}$.  The change is simply
$\Delta y_{igs} = y_{(i+1)gs} - y_{igs}$ .

Unfortunately, there are several difficulties with using this outcome
measure directly as a measure of uptake.  First, requesters can add
and subtract HITs over time. If a requester pulls down a large number
of HITs, then a regression might incorrectly lead us to believe that
there was something very attractive about the page position of that
HIT to workers.  Second, for 1-HIT wonders the HITs available measure
might not change, even though the HIT might be very popular. Finally,
the the absolute change is ``censored'' in that the number of HITs
available cannot go below zero, which means that groups with lots of
HITs available can  mechanically have larger changes than groups
with few HITs available. For these reason, we make our outcome
variable an indicator for a drop in HITs available, regardless of the
magnitude of the change:
\[Y_{igs} = 1\cdot \{\Delta y_{ijs} < 0\} \]

\subsection{Econometric set-up}
To implement our conceptual model, we will assume a linear regression
model, in which the expected outcome is a linear function of a series
of variables. Obviously, many factors determine whether or not a HIT
disappears.  Page, position and sort category matter but so do
variables that we don't observe and thus cannot include in a
regression. A high paying, quick task that looks fun will probably be
taken more quickly than one that is low paying, long and tedious.  We
want to separate the desirability of a task from the page and position
it occupies in a sort category.

\subsubsection{Choosing a model}
Rather than simply introducing a number of controls (which will
invariably miss factors that are unmeasured), we include a HIT
group-specific control in the regressions, called a group random
effect. This way, each group is modeled as having its own,
individual level of attractiveness that stays constant over time. In
this way, all factors that have a static effect on HIT
attractiveness are controlled for, regardless of whether or not we can
observe the factors that determine that group-specific effect.

With a group-specific random effect, we avoid what would otherwise be
seriously flawed analysis. For example, a fun HIT that always stays on
the third page of {\em highest reward} might erroneously cause us to
believe that the third page is very important, when in fact, it is
just that one HIT which is disappearing quickly. We will see this
exact issue when we compare the group specific random effects model to
the pooled model that lacks any group effects.

\subsubsection{Nomenclature}
One necessary word on the nomenclature of these two models: when
dealing with data that has a natural group structure like ours, a
model with group specific effect is called a random effects model or
multilevel model, while ignoring the group structure and not including
any group specific effect is called the ``pooled'' model. Both are
linear regression models. We should also note that while our group
specific effect model ``controls'' for group factors, we are not
actually including a dummy variable for each group (which is called
the ``fixed effects'' model), but rather are starting with a prior
that each effect is a random draw from a normal distribution (the
random effects model) \cite{gelman2007data}. As the number of
observations get large, as in our case, this fixed effects/random
effects distinction becomes less important.

\subsubsection{Model}
The group specific random effect model is as follows: for an
observation $i$ of group $g$, we estimate 
\begin{align} \label{eq:groups}
Y_{igs} = \sum_{r=1}^{30}\beta^r_s x^r_{igs} + \gamma_g + \tau_{H(i,g)} + \epsilon
\end{align} 
where the $x^r_{ig}=1$ if the HIT $i$ is at position $r$ (and
$x^r_{ig}=0$ otherwise), and where $\gamma_g \sim
N(0,\sigma_\gamma^2)$ is the group specific random effect and
$\tau_{H(i)} \sim N(0,\sigma^2_\tau)$ is a time of day random
effect. The ``pooled model'' ignores this grouped structure and
imposes the constraint that there is no group effect and $\gamma_g =
\gamma = 0$.

\subsection{Results}
\begin{figure*}
  \centering 
   \subfloat[][Model with HIT Group Specific Random Effects]{\includegraphics[scale=.5]{"./plots/FIGA_coef_re"}}
  \subfloat[][Pooled Model]{
    \includegraphics[scale=.5]{"./plots/FIGB_coef_pooled"}}
  \caption{Effects of position in sort category on HIT
    disappearance. Pages are illustrated with
    color. \label{fig:combined}}
\end{figure*} 

Of the twelve possible sort categories on MTurk, we choose to apply
these models to four of them: {\em newest} HITs, {\em most available}
HITs, {\em highest reward} and {\em title a-z}. The sort categories
that we are not looking at are ones that tend to produce 1-HIT wonders
and which have no clear reasoning for using them (such as {\em
  shortest} time allotted and {\em latest expiration}.)

In Figure \ref{fig:combined}, the collection of coefficients,
$\beta^r_s$ from Equation \ref{eq:groups} are plotted for both the
group specific random effects model (Panel A) and for the pooled model
(Panel B), with error bands two standard errors wide. The four sort
categories are across the top of the figure ({\em newest}, {\em most},
{\em highest reward}, {\em a-z}), the points are the coefficients
(with standard error bars) for each page and position on that page,
ranging from 1 to 30 (positions are displayed as negative numbers to
preserve the spatial ordering).  The coefficients $\hat{\beta}^r_s$
are interpretable as the probability that a HIT occupying position $r$
decremented by 1 or more HITs during the scraping interval. Our
premise is that categories, pages and positions with higher
probability of the HIT decremented by 1 or more HITs are the
categories, pages and positions that workers are searching by.


\subsection{Pooled model}
The group specific random effects results shown in Panel A in Figure
\ref{fig:combined} are the more credible estimates, but it is
interesting to compare them with the Panel B pooled estimates. First, we
can see that Panel B coefficients are far more precisely estimated,
compared to Panel A. This is because Panel A coefficients are
determined solely by within-group movements in position. For sort
categories not showing much movement---such as {\em Title}, the Panel
A estimates are very imprecise, while those in Panel B are very
precise---but all these estimates in B are capturing is innate
attractiveness of whatever HIT was occupying a particular
position. For example, at $r=20$ in Panel B, {\em Title}, we see a
strongly, statistically significant effect, but this is surely just a
feature of whatever HIT group was occupying that position.

This ``stationary HIT'' bias issue also arises in Panel B, {\em
  reward}: on the first page of search results for {\em highest},
there is almost zero probability of the HIT disappearing.  This is to
be expected.  Most HITs on MTurk have a reward between \$0.02 and
\$1.00.  However, there are some HITs that get posted for larger sums,
on the order of \$10.00.  Although there are very few of these HITs,
they tend to be unpopular, so they stay on the {\em highest reward}
page for a long time without disappearing.  We do see that on the
second and third results page, the probability of non-zero
disappearance rate goes up.

\subsection{Group random effects model} 
Under the group specific random effects model assumptions, we can
interpret the position coefficients as the probability that a HIT group
occupying that position will have a disappearance event during the
scraping interval. The only sort category showing strong positional
effects is {\em most}, with similar rates for the first and second
pages, and then a strong drop-off on the third page.

From this, we conclude that workers are actively sorting by the {\em
  most} available HITs and looking at results on the first two pages.
On the third page of search results for the {\em most} available HITs,
the average coefficient is high, but position effects seem to matter.
We conjecture this to mean that the further down the third page , the
more likely workers are to abandon this search strategy, but more
investigation would need to be done is confirm this interpretation.

The {\em highest reward} search category positions seems to have no
effect on search behavior. The overall level of disappearance is low
for HITs on this page, which is to be expected given that relative
unpopularity of high reward HITs. The page and positions within {\em
  title a-z} sort category seems to have no effect, which is
unsurprising.

\subsection{No time effects?}
According to Panel A, the position within {\em newest} HITs is
generally either irrelevant, or in the case of position 1, page 1, is
actually very harmful! This {\em is} surprising.  Sorting by the {\em
  newest} HITs seems to be a very good way to find fresh and
interesting tasks. Further, as we will see from our survey results,
most workers report searching for HITs based {\em newest}.

Interestingly, {\em newest} is a sort category where we would expect
the inclusion of group effects to be irrelevant, since position in the
{\em newest} sort category should be basically mechanical---a HIT
group is posted, starts at the first position and then marches
straight down the ranks as new HITs are added. Any yet comparing {\em
  newest} across Panels, the positional effects are radically
different: in Panel B, we see very strong positional effects, with the
predicted pattern .

\subsection{Discrepancy}
We will discuss the evidence below, but we will state our hypothesis
up front: the reason we no positional effects in the group specific
random effects model is that certain requesters actively game the
system by automatically updating the system. The gaming takes the form
of some requesters re-posting their HITs so that they will stay near
the to position on {\em newest}. This hypothesis would reconcile two
of our surprising findings.

First, gaming would explain the absence of effects {\em newest}
effects in Panel A: the benefits to being in a certain position are
captured as part of the group effect. To see why, consider a requester
that can always keep their HIT at position 1. No matter how valuable
position 1 is in attracting workers, all of this effect will be
(wrongly) attributed to that particular HIT's group-specific effect.

Second, gaming would explain why position 1 appears to offer far worse
performance in Panel A (and why there is a generally increasing trend
in coefficient size until position 4). A very large share of the HITs
observed at position 1 in the data come from the gaming
requesters. A still large but smaller share of the HITs observed at
position 2 come from gaming requesters, because gaming requesters in
position 2 are trying to get back to position 1, and so on. Because
all the purely positional benefits get loaded onto the group effects
of gaming requestors, positions more likely to be occupied by gaming
requestors will appear to offer smaller positional benefits. 

The gaming not only reconciles the data---it is also directly
observable.  We identified the HITs that had 75 or more observations
in position 1 of {\em newest}. In Figure \ref{fig:C}, the position
(1-30) is plotted over time with the group HIT ID and HIT title posted
above.  It it immediately obvious that some HITs are being updated
automatically so that they occupy a position near the top of the sort
category. Note that up until about day .5, the HIT group in the 2nd
and 3rd Panels (from the top) are ``competing'' for the top
spot. Around day .9, they face competition from a third HIT group
(1st panel) that pushes the other two down the rankings.

Under these gaming circumstances, the group specific random effects
model is inappropriate, as the positions are clearly not randomly
assigned.  For this reason, the pooled model is probably offering a
clearer picture of the positional effects, though these coefficient
estimates are certainly biased because they incorporate the nature of
the HITs occupying position 1, which we know is from gaming.

\begin{figure}
\centering
\includegraphics[scale=.4]{"./plots/FIGC"}
\caption{ Position of Three HITs over time sorted by {\em newest} \label{fig:C}} 
\end{figure}

\section{Method B: Worker Survey}
Another way to study how workers search for HITs is to ask the workers
directly, by running a survey on MTurk. Although this approach is more
expensive, more obtrusive, and smaller-scale than scraping the MTurk
web site, it has two advantages.  First, surveying complements the
scraping technique.  Scraping is better suited to tracking postings of
large numbers of HITs that can be done in any quantity by any worker,
so an anonymous scraper can observe their take-up.  Surveys, by
contrast, are necessarily 1-HIT wonders, offered at most once to every
worker, and MTurk offers no way for a scraper to observe how many
workers have taken up the offer.  Since a substantial use of MTurk
involves 1-HIT wonders for online experimentation and surveys of other
kinds, we need a method for exploring search behavior on this side of
the market.  Second, surveys can provide more detailed and subjective
explanations for worker behavior than aggregate statistics can.

This section presents the results of a survey of roughly 200 workers
about how they sort and filter HITs. Since MTurk itself was used to
conduct the survey, the characteristics of the HIT used to solicit
survey respondents have an effect on the results.  We explored this
effect by posting the survey several different ways, in order to
sample workers who used different search behaviors.  In the process,
we observed how certain choices of HIT characteristics can make a HIT
very hard to find, substantially reducing the response rate.  Finally,
the survey also collected free-form comments from workers about how
they find good and bad HITs.

\subsection{Procedure}
The survey was designed to probe the respondent's immediate search
behavior, specifically how the respondent used MTurk's search
interface to discover the survey HIT itself. As discussed earlier in
Section \ref{sec:features}, MTurk offers two ways to control the
display of HITs: sorting and filtering.  The first question asks about
sort order, using the same drop-down menu that MTurk itself presents,
for the sake of consistency and familiarity.  The second question asks
about filtering.  Although MTurk offers three ways to filter (price,
keywords, and qualifications), the survey asks only about price,
because the survey HIT required no qualifications and presumed that
the user wouldn't be looking for it directly using a keyword.  The
third question asks the worker for free-form comments.

The survey was posted on MTurk in three different ways, using
different HIT characteristics for each posting to give it good or bad
ranking under the different sort orders, as described below.  All
postings were made over a single 48-hour period on a weekend in early
May 2010.

\subsubsection{Best-case posting}
In this posting, the parameters of the HIT were chosen to place it at
an extreme of each of the six attributes available for sorting. Thus
the HIT would appear on the first page, or as close as possible, when
HITs were sorted by {\em any} attribute in either ascending or
descending order.  The goal of this posting was to capture as many
workers as possible from each of the six chosen sort orders that the
posting optimized.

For HIT creation time, the HIT was automatically one of the {\em
  newest} at first.  For HITs available, the survey was one of the
{\em fewest}, because it offered only one HIT to each user.  For
reward amount, we chose {\em lowest}, offering \$0.01.  This was not
the minimum, because some HITs are posted for \$0.00, but workers are
likely to be aware enough of prices to skip over work-for-nothing
tasks.  For the {\em soonest} expiration date and the {\em shortest}
time allotted, we chose 5 hours and 5 minutes, respectively. Again,
existing HITs had shorter deadlines, but our choices were constrained
by the requirements of the survey itself, and they were sufficient to
land the HIT on the first page of these two sort orders.  Finally, we
optimized for the {\em a-z} title order by starting the title with a
space character.

%\begin{figure*}[htp]
%\centering
%\includegraphics[scale=.6]{"../images/survey"}
%\caption{Survey about sorting and filtering on MTurk}
%\label{fig:X1}
%\end{figure*}

\subsubsection{Newest-only posting}
In this posting, the HIT was designed to appear only at the start of
{\em newest}, which we hypothesized was the most important sort order
for 1-HIT wonders.  To see if we could isolate the workers using this
sort order, we chose the remaining attributes to make the HIT as {\em
  hard to find} as possible for workers using other sort orders.  This
was done by choosing parameter values near the median value of
existing HITs on MTurk, so that the survey HIT would appear in the
middle of the pack---and hence as far as possible from the first page
in both ascending and descending order by that attribute.

For example, at the time the survey was posted, the median value for
HITs available was 2.  Postings with 2 HITs covered pages 55--65 (out
of 100 pages of HITs requiring no qualifications) when the list was
sorted by {\em fewest} HITs available, and pages 35--45 when sorted by
{\em most} HITs.  As a result, a worker sorting in either direction
would have to click through at least 35 pages to reach any 2-HIT
posting, which is highly unlikely.

The survey was therefore posted with 2 independent HITs.  As a result,
MTurk no longer guaranteed that a worker could only answer it once.
We enforced this instead by placing a cookie on the worker's browser
after survey submission, but some workers evaded this, and their
redundant answers were discarded post hoc by searching for duplicate
MTurk worker IDs in the submitted survey.

For the remaining attributes, we chose reward amount \$0.05,
expiration date 1 week, and time allotted 60 minutes, each of which
guaranteed placement of the HITs at least 20 pages deep in both
ascending and descending order.  The median title started with U, so
we prefixed the title with the word ``Urgent" to make it as hard to
find as possible using both {\em a-z} and {\em z-a} sorting.

\subsubsection{Worst-case  posting}
Finally, to test the impact of truly bad position on a HIT's
reachability, we posted the survey in a way that put it on a deep page
for {\em all} sort orders.  For the five attributes other than
creation date, the HIT parameters were the same as just described for
newest-only posting.

Creation date cannot be chosen directly, however; in particular, a HIT
cannot be posted with a creation date in the past.  Freshly posted
HITs generally appear on the first page of the {\em newest} sort.  In
order to artificially age our survey HIT before making it available to
workers, we initially posted a nonfunctional HIT, which presented a
blank page when workers previewed it or tried to accept it.  After two
hours, when the HIT had fallen more than 5 pages deep in the {\em
  newest} sort order, the blank page was replaced by the actual
survey, and workers who discovered it from that point on were able to
complete it.

Each of three postings was kept on MTurk until it obtained 100 answers
to the survey or 5 hours had elapsed, whichever came first.

\subsection{Results}
Altogether, the three postings recruited 234 unique workers to the
survey: 100 by the best-case posting, 86 by the newest-only posting
(14 were removed as duplicate submissions by the same worker), and 38
by the worst-case posting.  Only the worst-case posting was unable to
collect 100 answers to the survey in 5 hours.

Figure \ref{fig:X2} shows the rate at which each posting recruited workers to
the survey.  The best-case and newest-only postings have similar
curves, which suggest an initially steep response rate (while the
posting is on the first page of the {\em newest} sort order), followed
by a slower take-up as the posting falls onto deeper pages.  The
worst-case posting, by contrast, has a steady but low response rate,
recruiting only 8 workers per hour on average, compared to 25 workers
per hour for the other two postings, almost certainly because it
started out deep in the page list for all sort orders.

Figure \ref{fig:X3} shows the responses to the sort order question of
the survey, grouped by the posting that solicited the response.
Looking first at newest-only shows that this posting successfully
recruited workers who sort by {\em newest}; over 80\% of the responses
in this group were in this category.  The {\em newest} sort order
dominated in the other groups, followed by {\em most} HITs.  Strong
appearance of {\em most} HITs was a surprise.  Recall that the
best-case posting tried to attract workers who sorted not only by {\em
  newest}, but also {\em fewest} HITs, {\em lowest} price, {\em a-z},
{\em soonest} expiration, and {\em shortest} time allotted.  All these
sort orders appeared in the responses, but {\em fewest} was still
strongly dominated by {\em most}.

For the price filter question, roughly 80\% of the survey respondents
reported using no minimum price when they found the survey HIT.
Surprisingly, however, 8\% reported using a price filter that was {\em
  greater} than the reward offered by the survey HIT.  For example,
even though the best-case posting offered only \$0.01, ten respondents
to it reported finding it while searching for HITs that paid at least
\$0.05.  We followed up by sending email (through MTurk) to several of
these workers, and found that the problem can be traced to a usability
bug in MTurk:

{\em ``Once we submit a hit MTurk takes us back to all hits available,
1st page with no \$ criteria."
}

In other words, the filter is discarded as soon as the worker does a single task.  Several other workers mentioned similar problems in their free-form comments.

The free-form comment question generated a variety of feedback and ideas.  A commonly-cited concern were ``scams," by which workers typically meant a profusion of high-reward tasks that ask workers to sign up for paid services or provide personal information that might be abused.  Twelve workers asked for the ability to search for, or filter out, postings from particular requesters, e.g.:

{\em ``Pages and pages and pages from the same requester that I know I won't choose are annoying because I have to scroll through those pages to get to new hit I'd like to do.  I'd like a way to exclude these requesters when searching for a hit."
}

Other suggestions included the ability to group HITs by type, and to offer worker-provided ratings of HITs or requesters.

Several workers mentioned using keyword search to find desirable HITs, particularly keywords like ``fun", ``quick", and ``survey." In fact, the use of ``survey" in the title of the HIT may have boosted the response rate, particularly for the worst-case posting which is hard to find any other way. However:

{\em ``It would be a great search system, but a lot of tasks are bloated with non-applicable search terms (such as tagging it with `survey' when it has nothing to do with it."
}

Several workers specifically mentioned the burden of clicking through pages to find good HITs:

{\em ``Scrolling through all 8 pages to find HITs can be a little tiring."}

{\em ``I find it easier to find hits on mechanical turk if I search for the newest tasks created first.If I don't find anything up until page 10 then I refresh the page and start over otherwise it becomes too hard to find tasks."}

Two workers pointed out the usability bug in MTurk that makes this problem even worse:

{\em ``It is difficult to move through the pages to find a good HIT because as soon as you finish a HIT, it automatically sends you back to page 1. If you are on page 25, it takes so much time to get back to page 25 (and past there)."}


\begin{figure}[htp]
\centering
\includegraphics[scale=.5]{"./plots/rcm_combined"}
\caption{Number of workers who reported using each sort order to find
  each of the three postings}
\label{fig:X3}
\end{figure}


\begin{figure}[htp]
\centering
\includegraphics[scale=.5]{"./plots/rcm_timing"}
\caption{Number of workers taking the survey over time}
\label{fig:X2}
\end{figure}

%
%\begin{figure}[htp]
%\centering
%\includegraphics[scale=.4]{"../images/allthree"}
%\caption{Number of workers recruited over time by each of the three postings.}
%\label{fig:X2}
%\end{figure}

%\begin{figure}[htp]
%\centering
%\includegraphics[scale=.4]{"../images/bestnewestworst"}
%%actually this needs to be one image with all three graphs
%\caption{
%\end{figure}

\subsection{Findings}


Our survey was posted with parameters to target search strategies that the scraped data may overlook.  For example, one HIT-wonders and HITs buried deep in the results.  In both the survey results and the scraped data results, we find
workers searching by {\em newest} HITs.  However, although our two
methods are complementary, we don't necessarily expect to see
similarities.  For example, in the scraped data, we observe workers
searching by {\em most} HITs however we don't see workers searching by
the {\em most} HITs in the survey because it would be nearly
impossible to find our survey while searching under {\em most} HITs.  In the scraped data we found evidence of workers sorting by {\em newest} HITs and by {\em most} HITs.  However, based on the survey
results, we see that workers report using 10 of the 12 sort
categories.  With a larger survey sample, it is possible that we could
find evidence of workers sorting by all the 12 categories.

Surprisingly, we see that workers looking fairly deep into the results
pages.  This is evident both by the free-text response and the
experiments with posting HITs 5 pages deep into the {\em newest} HITs
and observing workers complete them.  From the scraped data, we
conclude that looking past the third page is not common, but from the
survey we can safely conclude that it does happen sometimes.

Workers mentioned that in the process of searching for {\em newest}
HITs, they would wait some length of time and revisit the first page
of {\em newest} HITs.  This seems to be an effective tactic and bears
much similarity to how users browsing the web will revisit sites
looking for content that has changed.  Tools have been developed to
aid the recognition of updated web content \cite{teevan2009changing,
  adar2008zoetrope}, perhaps similar tools for task search would be
valuable.


\section{Conclusion}
In this paper, we studied the search behavior of MTurk workers
using two different methods.  By scraping pages listing available
tasks at a very high rate, we used the rate of disappearance of HITs
to see if one method of sorting resulted in tasks being taken more
quickly than others.  

Because this method does not accurately measure all types of HITs, we
also posted a survey on MTurk workers asking them how they searched
for our survey task.  We posted the task with specially chosen
parameters complementary to the parameters of the HITs we gathered
information on using the scraper.

Both methods agree that workers tend to sort by the {\em newest} HITs.
The scraping revealed that workers also sort by the {\em most} HITs,
and focus mainly on the first two pages of results but ignoring the position of the HITs on the page.  The survey data
confirms that even though {\em newest} and {\em most} are the most
popular search strategies, the other sort categories are also used -
even obscure ones such as {\em title a-z} and {\em oldest} HITs, but
to a much lesser extent.  The survey gives evidence of some workers
willing to wade fairly deep into the results page and willing to
return the {\em newest} HITs periodically hoping for the next new
HITs.

\section{Future Work}
Our analysis of search behavior does not account for keyword search
and sorting by qualification.  Although workers in the survey
mentioned that searching by keyword was a flawed method of finding
desirable HITs, we could use our scraping method to determine the
effect of having certain keywords in a HIT on the disappearance rate
of the HIT.

\bibliographystyle{abbrv}
\bibliography{search}

\end{document} 



\section{John's old section}
\subsection{Empirical Framework} 
We collect data on a number of HIT characteristics that surely affect
how desirable workers find those HITs. Everything else being equal,
workers presumably prefer HITs that pay more, take less time, are
interesting etc. However, it would be a mistake to regress some measure
of desirability---such as task uptake---on HIT characteristics 
and then give the resultant model a causal interpretation. 

The problem is that HIT traits are not randomly determined and
independent; they are highly correlated with factors that we are
unable to observe and yet we can be certain affect uptake. For
example, HITs paying more money are more attractive all else equal,
but HITs pay more money precisely when the work is more onerous and
time-consuming.  

Because we do not observe task difficulty, the requester's reputation,
and the skills required to complete a HIT, the so-called ``omitted
variable'' problem will be hard to overcome using purely observational
data like the data obtained from scraping.\footnote{Econometric
  difficulties aside, the best way to determine the valuation workers
  put on different job attributes is to conduct randomized experiments
  where the experimenter manipulates HIT characteristics and then
  observes the differences in views and uptakes.} However, it is still
possible to learn a great deal from the data by choosing a model that
can deal with nature of the data.

Although we are limited by the face that unobserved (to us)
characteristics affect output, one major advantage of our data is that
by using the high-frequency scraper, we obtain multiple (in some
cases, thousands) of observations on a single HIT group. If we believe
that the idiosyncratic factors specific to a HIT group are unchanging,
then our repeated measurement allow us to estimate the effects of HIT
group factors that \emph{do} change over time.

Fortunately, those factors that do change over time are the ones of
greater interest. The question we cannot readily answer---how
idiosyncratic HIT features affect uptake---is less interesting than
the question that we can answer, which is how the search technology
and procedures itself---manifested in the page and position of a HIT,
over time, in different ``views''---affects uptake. Before we discuss
our method in detail, it is necessary to provide some background on
dealing with data structured like ours and how to interpret the models
appropriate for such data.

\subsection{Multilevel Models} 
The data collected from the scraper has a strongly grouped structure,
with observations that are clearly not independent draws. Until a HIT
moves off one of the scraped pages, each HIT collection is observed by
the scraper approximately every 30 seconds. HIT collections are thus
generally observed many times. Each observation in the group shares
certain time-invariant characteristics, such as the identity of the
requester, the title, the description and the reward. However, other
factors, like the number of HITs available or the page and the
position change over time.

\subsection{Defining Variables} 
Our outcome of interest is rate at which work is completed. One way to
measure this outcome is to look how the number of HITs available
within a particular group changes over time.  Let $s$ index the search
view, $g$ index the groups of observations of the same HIT and let $i$
index time-ordered observations within a group. Let HITs be ordered
from oldest to most recently scraped, such that $i+1$ was scraped
after $i$. Let number of HITs available for observation $i$ be
$y_{igs}$.  The change is simply $\Delta y_{igs} = y_{(i+1)gs}$
. Unfortunately, there are several problems with using this outcome
measure directly as a measure of uptake.

First, requesters can add and subtract HITs over time. If a requester
pulls down a large number of HITs, then a regression might incorrectly
lead us to believe that there was something very attractive about the
page position of that HIT to workers.  Second, when a requester wants
each HIT done multiple times by different workers, the HITs available
measure might not change, even though the HIT might be very
popular. Third, is decremented when a worker accepts a HIT; if that
worker then returns at HIT before completing it, then our outcome is
in some sense a horse-race between HITs being accepted and HITs being
returned. Finally, the the absolute change is ``censored'' in that the
number of HITs available cannot go below zero, which means that groups
with lots of HITs available can have mechanically have larger changes
than groups with few HITs available.
      
These problems further highlight the need to use group-specific
effects. They are also justification for making our outcome variable
an indicator for a drop in HITs available: 
\[Y_{igs} = 1\cdot \{\Delta y_{ijs} < 0\} \]

The outcome now interpretable as more HITs were accepted than were
returned between the scraping interval. Part of the effect of
requesters adding/subtracting HITs will be captured by the
group-effect, while the rest will at least be ``contained'' in the
sense that even if a requester added thousands of HITs, this action
would not swamp the effect of legitimate changes in HITs available due
to acceptance.

To create the , we ordered all the HITs within a group by scrape time,
and then computed the elapsed time between successive observations for
that group (30 seconds between observations) and computed the change
in the number of HITs available.

\subsection{The Model} 
For an observation $i$ of group $g$, and for a give page $p$ and
search view $s$, we estimate the multilevel model with fixed effects
for a HIT's position on the page ($x^r_{ig}=1$ is the HIT $i$ is at
position $r$, $x^r_{ig}=0$ otherwise) and random effects for the group, $\gamma_{ig}$ and
for the scrape hour (0-24), $\tau_{H(i,j)}$:
\begin{align} \label{eq:mlm}
Y_{ig} = \sum_{r=1}^{10}\beta^r_s x^r_{ig} + \gamma_{g} + \tau_{H(i,g)} + \epsilon
\end{align} 
where $\gamma_g \sim N(0,\sigma^2)$ and $\tau_{H(i)} \sim N(0,\sigma)$. 

When we include group-specific effects, the effects of position and
page can only be identified via within-group changes in position and
page: if a particular group always stayed at a position, then whatever
effect on uptake was due solely to that position would be ``absorbed''
by the group-specific effect.

\subsection{Results} 
The coefficients $\hat{\beta}^r_s$ from Equation \ref{eq:groups} is
interpretable as the probability that a HIT group occupying position
$r$ decremented by 1 or more HITs available during the scraping
interval. The results of this regression, from pages $p=1\ldots3$ and
for four difference search views are shown in in Figure \ref{fig:A}. 

The red vertical line is the average value of the coefficient. The
search categories are in columns, while the pages are in rows. The
coefficients from the associated regression are plotted in each
panel. Overall, with the exception of \verb|NumHITsAvailable|,
there appear to be rather weak page and position effects.

There is perhaps a slight advantage to being listed first
alphabetically. Being in the position of having the most HITs
available has a strong, positive effect.

The \verb|Reward1| coefficients are precisely estimated
zeros. The reason is that there is almost no variation in the outcome
variable for these high-valued HIT groups. Only about 5 out of 10,000
observations had any of these HITs being decremented. In a case like
this, the random effects model is explaining whatever idiosyncratic
variation exists. Essentially, there cannot be strong positional
effects in Reward1on uptake because there is no uptake.

The big surprise is the lack of position or page effects in the
\verb|LastUpdatedTime1| column. Anecdotally, requesters and workers
alike seem to believe that being a ``new'' HIT has an effect on
uptake. That HITs in the first position of \verb|LastUpdatedTime1| are
disappearing at the same rate as those in Title1 seems implausible. To
help understand this puzzle, we will first look at the pooled,
ordinary regression that excludes group effects. While these
regressions cannot be interpreted causally, they coefficients do give
the mean number of decrements in that position. 

In Figure \ref{fig:B}, we report the coefficients of the pool
regression. In the Last Updated Column, we can now see large effects
associated with both page and rank within the page. On average, the
first listed HIT on page 1 of the LastUpdatedTime1 was decremented
about 30\% of the time, compared to about 10\% on page 3, position 10.

\subsubsection{Reconciling the Anomoly}
Either their truly are no positional effects or there is something
wrong with the model and its assumptions. Given the survey evidence
[xxx  introduced yet?], it seems unlikely that position does not
matter in the LastUpdatedTime1 search view. One way that the model
assumptions could be violated is a certain HIT collection spends more
time in a certain position than random chance would permit. One way
this could happen is if requesters are able to control their
position. If, for example, a requester can move into position 1 often,
and there are true advantages to being in position 1, then this purely
positional advantage gets absorbed by the group specific effect. We
wrongly conclude that a HIT is getting done a lot because of the
nature of the HIT, when in fact the ``nature of the HIT'' is to often
be near the top of the list.



\subsection{Economics}
There is a large and influential literature on job search in economics
[Mortensen and Pissarides, 1999]. The basic question is how workers
search for jobs that will be a good fit for their skills and offer
more benefits than their next best alternative? Research on job
selection is important to the design of unemployment insurance and
other social policies, but unfortunately, traditional job search is
difficult to study.  We observe people getting a job and leaving a
job, but we don't get to see them actually searching or observe which
jobs they considered and rejected (or jobs they were rejected for).
MTurk provides a unique window into this process.  Although MTurk is
very different from a traditional labor market, search strategies may
bear similarities, especially with other online labor markets such as
oDesk and Elance.  Furthermore, if our understanding of search
behavior on MTurk leads us to having an impact on efficiency, fairness
and social welfare of online labor markets, and it may help human
computation and online labor markets gain mainstream acceptance.
